{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "\n",
    "import taxi_env as te\n",
    "import taxi_util as tu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Successfully Initialized!\n"
     ]
    }
   ],
   "source": [
    "#define the input here\n",
    "N_station=84\n",
    "l1=[5 for i in range(N_station)]\n",
    "OD_mat=[l1 for i in range(N_station)]\n",
    "distance=OD_mat\n",
    "travel_time=OD_mat\n",
    "arrival_rate=[1 for i in range(N_station)]\n",
    "taxi_input=6\n",
    "\n",
    "env=te.taxi_simulator(arrival_rate,OD_mat,distance,travel_time,taxi_input)\n",
    "env.reset()\n",
    "print('System Successfully Initialized!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the structure of the deep Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,N_station,h_size,rnn_cell,myScope):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        \n",
    "        #input is a scalar which will later be reshaped\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,N_station*N_station*3],dtype=tf.float32)\n",
    "        \n",
    "        #input is a tensor, like a 3 chanel image\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,N_station,N_station,3]) \n",
    "        \n",
    "        \n",
    "        #create 4 convolution layers first\n",
    "        \n",
    "        self.conv1 = slim.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,\\\n",
    "            kernel_size=[8,8],stride=[4,4],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv1')\n",
    "        self.conv2 = slim.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,\\\n",
    "            kernel_size=[3,3],stride=[2,2],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv2')\n",
    "        self.conv3 = slim.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,\\\n",
    "            kernel_size=[3,3],stride=[1,1],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv3')\n",
    "        self.conv4 = slim.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,\\\n",
    "            kernel_size=[7,7],stride=[1,1],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv4')\n",
    "        \n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)\n",
    "        #We take the output from the final convolutional layer and send it to a recurrent layer.\n",
    "        #The input must be reshaped into [batch x trace x units] for rnn processing, \n",
    "        #and then returned to [batch x units] when sent through the upper levles.\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "        self.convFlat = tf.reshape(slim.flatten(self.conv4),[self.batch_size,self.trainLength,h_size])\n",
    "        self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=self.convFlat,cell=rnn_cell,dtype=tf.float32,initial_state=self.state_in,scope=myScope+'_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "        #The output from the recurrent player is then split into separate Value and Advantage streams\n",
    "        self.streamA,self.streamV = tf.split(self.rnn,2,1)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,N_station]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        self.salience = tf.gradients(self.Advantage,self.imageIn)\n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,N_station,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "        #In order to only propogate accurate gradients through the network, we will mask the first\n",
    "        #half of the losses for each trace as per Lample & Chatlot 2016\n",
    "        self.maskA = tf.zeros([self.batch_size,self.trainLength//2])\n",
    "        self.maskB = tf.ones([self.batch_size,self.trainLength//2])\n",
    "        self.mask = tf.concat([self.maskA,self.maskB],1)\n",
    "        self.mask = tf.reshape(self.mask,[-1])\n",
    "        self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay\n",
    "\n",
    "These classes allow us to store experies and sample then randomly to train the network.\n",
    "Episode buffer stores experiences for each individal episode.\n",
    "Experience buffer stores entire episodes of experience, and sample() allows us to get training batches needed from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self,batch_size,trace_length):\n",
    "        sampled_episodes = random.sample(self.buffer,batch_size)\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0,len(episode)+1-trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces,[batch_size*trace_length,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "Network parameter here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the training parameters\n",
    "batch_size = 4 #How many experience traces to use for each training step.\n",
    "trace_length = 8 #How long each experience trace will be when training\n",
    "update_freq = 5 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 10000 #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 1000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 1000 #How many steps of random actions before training begins.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./drqn\" #The path to save our model to.\n",
    "h_size = 128 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "time_per_step = 1 #Length of each step used in gif creation\n",
    "summaryLength = 100 #Number of epidoes to periodically save for analysis\n",
    "tau = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set Success\n",
      "Episode: 0 , totalreward: 45.8571428480442\n",
      "Episode: 1 , totalreward: 46.14484126068554\n",
      "Episode: 2 , totalreward: 46.23015872098609\n",
      "Episode: 3 , totalreward: 46.224206340034854\n",
      "Episode: 4 , totalreward: 46.386904752700985\n",
      "Episode: 5 , totalreward: 46.40079364158715\n",
      "Episode: 6 , totalreward: 45.71825395918288\n",
      "Episode: 7 , totalreward: 46.51388887965992\n",
      "Episode: 8 , totalreward: 46.365079355879914\n",
      "Episode: 9 , totalreward: 46.04365078451518\n",
      "Episode: 10 , totalreward: 46.54563491139969\n",
      "Episode: 11 , totalreward: 45.55555554651676\n",
      "Episode: 12 , totalreward: 46.03373014959649\n",
      "Episode: 13 , totalreward: 46.03571427658023\n",
      "Episode: 14 , totalreward: 46.16865078449035\n",
      "Episode: 15 , totalreward: 45.9027777686701\n",
      "Episode: 16 , totalreward: 46.19841268924636\n",
      "Episode: 17 , totalreward: 46.66865078439113\n",
      "Episode: 18 , totalreward: 46.17857141940901\n",
      "Episode: 19 , totalreward: 46.42460316539195\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 20 , totalreward: 45.8273809432882\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 21 , totalreward: 46.47420633998528\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 22 , totalreward: 46.001984117856765\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 23 , totalreward: 45.90277776867008\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 24 , totalreward: 46.25793649875836\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 25 , totalreward: 46.398809514603414\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 26 , totalreward: 45.98809522897064\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 27 , totalreward: 45.88293649883276\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 28 , totalreward: 46.57936507012314\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 29 , totalreward: 46.317460308270356\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 30 , totalreward: 46.45634919713165\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 31 , totalreward: 46.03373014959649\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 32 , totalreward: 46.2480158638397\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 33 , totalreward: 46.02777776864529\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 34 , totalreward: 46.180555546392746\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 35 , totalreward: 45.92460316549116\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 36 , totalreward: 46.249999990823426\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 37 , totalreward: 45.926587292474906\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 38 , totalreward: 46.228174594002354\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 39 , totalreward: 45.92460316549114\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 40 , totalreward: 45.8611111020117\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 41 , totalreward: 45.9861111019869\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 42 , totalreward: 46.35515872096129\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 43 , totalreward: 46.0694444353037\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Target Set Success\n",
      "Episode: 44 , totalreward: 45.5694444354029\n",
      "Target Set Success\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-23a304a83bc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mtrainBatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyBuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrace_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Get a random batch of experiences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0;31m#Below we perform the Double-DQN update to the target Q-values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mQ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m                        \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainLength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrace_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mQ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m                        \u001b[0mtargetQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mtargetQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainLength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrace_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargetQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargetQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mdoubleQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#We define the cells for the primary and target q-networks\n",
    "\n",
    "#LSTM First\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "\n",
    "#DQN combined with LSTM\n",
    "mainQN = Qnetwork(N_station,h_size,cell,'main')\n",
    "targetQN = Qnetwork(N_station,h_size,cellT,'target')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "trainables = tf.trainable_variables() #return the list of variables to train\n",
    "\n",
    "targetOps = tu.updateTargetGraph(trainables,tau) #match the variables of target network with the origin one\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "##Write the first line of the master log-file for the Control Center\n",
    "with open('./Center/log.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['Episode','Length','Reward','IMG','LOG','SAL'])    \n",
    "  \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #this step loads the model from the model that has been saved\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "   \n",
    "    #this example equals target network to the original network after every few episodes\n",
    "    #we may want to modify this\n",
    "    \n",
    "    tu.updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = []\n",
    "        \n",
    "        #Reset environment and get first new observation\n",
    "        env.reset() \n",
    "        \n",
    "        #return the current state of the system\n",
    "        sP,tempr=env.get_state() \n",
    "        #process the state into a list\n",
    "        s = tu.processState(sP,N_station)\n",
    "        \n",
    "        \n",
    "        rAll = 0\n",
    "        j = 0\n",
    "\n",
    "        #Reset the recurrent layer's hidden state\n",
    "        state = (np.zeros([1,h_size]),np.zeros([1,h_size])) \n",
    "        #The Q-Network\n",
    "        while j < max_epLength: \n",
    "            j+=1\n",
    "            \n",
    "            #for all the stations, act greedily\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                state1 = sess.run(mainQN.rnn_state,\\\n",
    "                    feed_dict={mainQN.scalarInput:[s],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = np.random.randint(0,N_station)\n",
    "            else:\n",
    "                a, state1 = sess.run([mainQN.predict,mainQN.rnn_state],\\\n",
    "                    feed_dict={mainQN.scalarInput:[s],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "            \n",
    "            #move to the next step based on action selected\n",
    "            env.step([a])\n",
    "            #get state and reward\n",
    "            s1P,r=env.get_state()\n",
    "            r=r/(taxi_input*N_station+0.0000001)\n",
    "            s1=tu.processState(s1P,N_station)\n",
    "            \n",
    "            total_steps += 1\n",
    "            \n",
    "            #episode buffer\n",
    "            #we don't store the initial 200 steps of the simulation, as warm up periods\n",
    "\n",
    "            episodeBuffer.append(np.reshape(np.array([s,a,r,s1]),[1,4]))\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "            #start training here\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    tu.updateTarget(targetOps,sess)  #update the target Q networks\n",
    "                    \n",
    "                    #Reset the recurrent layer's hidden state\n",
    "                    state_train = (np.zeros([batch_size,h_size]),np.zeros([batch_size,h_size])) \n",
    "                    trainBatch = myBuffer.sample(batch_size,trace_length) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={\\\n",
    "                        mainQN.scalarInput:np.vstack(trainBatch[:,3]),\\\n",
    "                        mainQN.trainLength:trace_length,mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={\\\n",
    "                        targetQN.scalarInput:np.vstack(trainBatch[:,3]),\\\n",
    "                        targetQN.trainLength:trace_length,targetQN.state_in:state_train,targetQN.batch_size:batch_size})\n",
    "                    doubleQ = Q2[range(batch_size*trace_length),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ)\n",
    "                    #Update the network with our target values.\n",
    "                    sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ,\\\n",
    "                        mainQN.actions:trainBatch[:,1],mainQN.trainLength:trace_length,\\\n",
    "                        mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "            \n",
    "            #update reward\n",
    "            rAll += r\n",
    "            \n",
    "            #swap state\n",
    "            s = s1\n",
    "            sP = s1P\n",
    "            state = state1\n",
    " \n",
    "\n",
    "        #Add the episode to the experience buffer\n",
    "        bufferArray = np.array(episodeBuffer)\n",
    "        episodeBuffer = list(zip(bufferArray))\n",
    "        myBuffer.add(episodeBuffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll) #reward in this episode\n",
    "        print('Episode:',i,', totalreward:',rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 100 == 0 and i != 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model\")\n",
    "        if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "            print (total_steps,np.mean(rList[-summaryLength:]), e)\n",
    "#             saveToCenter(i,rList,jList,np.reshape(np.array(episodeBuffer),[len(episodeBuffer),5]),\\\n",
    "#                 summaryLength,h_size,sess,mainQN,time_per_step)\n",
    "    #saver.save(sess,path+'/model-'+str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
